{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e19209d2-5eb3-457b-9ba1-7b63f10c8a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MP\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing libraries\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from  tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import  pathlib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099db985-4468-4ce6-8312-3e5d8cb3bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_WIDTH = 224\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51afc5be-9c06-4ba3-8335-9f3c509b4428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27153 files belonging to 10 classes.\n",
      "Using 21723 files for training.\n",
      "Found 27153 files belonging to 10 classes.\n",
      "Using 5430 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#importing dataset \n",
    "ds_train = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"C:/Users/MP/Videos/IMG_CLASSES\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    shuffle=True,\n",
    "    validation_split = 0.2,\n",
    "    subset = 'training',\n",
    "    seed = 123\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"C:/Users/MP/Videos/IMG_CLASSES\",\n",
    "    labels='inferred',\n",
    "    label_mode='categorical',\n",
    "    color_mode='rgb',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=(IMAGE_HEIGHT, IMAGE_WIDTH),\n",
    "    shuffle=False,\n",
    "    validation_split=0.2,\n",
    "    subset='validation',\n",
    "    seed=123\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48d19c0-7676-4e36-8169-a4156946e8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of classes \n",
    "class_names = ds_train.class_names\n",
    "num_classes = len(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5c13a56-d01a-4ce0-8ea8-c68a866ede3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data augumentation \n",
    "\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.05),\n",
    "])\n",
    "\n",
    "#data augmentation function \n",
    "\n",
    "def augment(image, label):\n",
    "    image = data_augmentation(image, training=True)\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac3c2da-0938-438f-839b-1fe6f65af228",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning using mobileNetV2\n",
    "IMG_SHAPE = (224, 224, 3)\n",
    "\n",
    "base_model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e615336-49dc-48c7-bc14-e9f30f9bae03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "\n",
    "def preprocess(image, label):\n",
    "    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85f2223f-edcc-40e3-92b6-95adaf2e51c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# TRAIN: augmentation + preprocess\n",
    "ds_train = ds_train.map(augment, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "ds_train = ds_train.prefetch(AUTOTUNE)\n",
    "\n",
    "# VALIDATION: preprocess ONLY (no augmentation)\n",
    "val_ds = val_ds.map(preprocess, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f57320ee-b42a-474d-8acc-b0ec76120db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze last 20 layers only\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-30]:\n",
    "    layer.trainable =False\n",
    "\n",
    "\n",
    "# Freeze BatchNorm layers\n",
    "for layer in base_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91932977-b3db-465d-9d36-f8243b541e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m885s\u001b[0m 1s/step - accuracy: 0.4980 - loss: 1.4260 - val_accuracy: 0.4560 - val_loss: 1.5321\n",
      "Epoch 2/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m793s\u001b[0m 1s/step - accuracy: 0.6390 - loss: 0.9402 - val_accuracy: 0.5435 - val_loss: 1.1735\n",
      "Epoch 3/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m812s\u001b[0m 1s/step - accuracy: 0.6775 - loss: 0.8476 - val_accuracy: 0.6142 - val_loss: 1.0217\n",
      "Epoch 4/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m792s\u001b[0m 1s/step - accuracy: 0.7081 - loss: 0.7704 - val_accuracy: 0.5593 - val_loss: 1.1276\n",
      "Epoch 5/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m715s\u001b[0m 1s/step - accuracy: 0.7337 - loss: 0.7081 - val_accuracy: 0.5378 - val_loss: 1.4266\n",
      "Epoch 6/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m897s\u001b[0m 1s/step - accuracy: 0.7415 - loss: 0.6801 - val_accuracy: 0.7055 - val_loss: 0.7746\n",
      "Epoch 7/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m932s\u001b[0m 1s/step - accuracy: 0.7683 - loss: 0.6263 - val_accuracy: 0.5028 - val_loss: 1.5601\n",
      "Epoch 8/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m833s\u001b[0m 1s/step - accuracy: 0.7832 - loss: 0.5825 - val_accuracy: 0.4674 - val_loss: 1.9855\n",
      "Epoch 9/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m948s\u001b[0m 1s/step - accuracy: 0.7976 - loss: 0.5510 - val_accuracy: 0.6053 - val_loss: 1.1699\n",
      "Epoch 10/10\n",
      "\u001b[1m679/679\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m939s\u001b[0m 1s/step - accuracy: 0.8181 - loss: 0.4963 - val_accuracy: 0.5134 - val_loss: 1.7311\n"
     ]
    }
   ],
   "source": [
    "#training the model \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer((224,224,3)),\n",
    "    base_model,\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "history = model.fit(\n",
    "    ds_train,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257b2314-cf03-4328-9808-afd9845e90ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
